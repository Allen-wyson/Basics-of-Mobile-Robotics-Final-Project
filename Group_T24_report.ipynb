{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f842d154",
   "metadata": {},
   "source": [
    "# **Mobile Robotics Projet**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3dcd3d",
   "metadata": {},
   "source": [
    "##### **Team 24 :**\n",
    "- Yuheng He\n",
    "- Alvaro Martinez-Vizmanos\n",
    "- Yo-Shiun Cheng\n",
    "- Lucas Jacques Louis Milesi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f309bd16",
   "metadata": {},
   "source": [
    "## **Presentation of the Project**\n",
    "We had to carry out a project in which we used the five robotics components studied during the semester:\n",
    "\n",
    "- Vision\n",
    "- Global Navigation\n",
    "- Motion Control\n",
    "- Local Navigation\n",
    "- Filtering\n",
    "\n",
    "In this project, we had to guide our Thymio robot from a starting point to a finishing point. It needed to follow an optimal path and avoid obstacles. Additionally, it had to be capable of dodging dynamic obstacles (those introduced along its path). To achieve this, we used a camera positioned above the environment where the Thymio robot operates.\n",
    "\n",
    "#### **Vision Part**\n",
    "This camera provided all the vision data necessary to detect the environment's boundaries, determine the robot's position and angle of rotation, and identify obstacles.\n",
    "\n",
    "#### **Global Navigation Part**\n",
    "Using the vision data, we extracted a binary image of the environment in which obstacles and clear paths were clearly identified. Based on this image, we applied the A* Dijkstra algorithm to find the optimal path between the starting and finishing points, as introduced in Lesson 5 of the course.\n",
    "\n",
    "#### **Motion Control Part**\n",
    "To control the robot, we implemented a Differential Drive Motion Control system as studied in Lesson 1 of the course. Specifically, we decided to use an Astolfi Controller, which we deemed suitable for our project.\n",
    "\n",
    "#### **Local Navigation Part**\n",
    "To enable the robot to dodge dynamic obstacles, we implemented a neural network connecting the laser sensors to the motors, as covered in Lesson 3 of the course. Local obstacle avoidance was used in parallel with motion control.\n",
    "\n",
    "#### **Filtering Part**\n",
    "To locate our robot precisely, we used an Extended Kalman Filter (EKF). This approach helps reduce noise and inaccuracies in sensor measurements, providing a more reliable estimate of the robot's position and orientation over time. Additionally, the EKF corrects cumulative errors, such as those caused by wheel slippage or sensor drift, which could otherwise lead to deviations in the robot's trajectory.\n",
    "\n",
    "<br>\n",
    "To complete the project in the time available, we divided up the various tasks between the different members of the group. It was necessary to have good communication between the different parts, as they are all interconnected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31499e86",
   "metadata": {},
   "source": [
    "## **Environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783c7cc3",
   "metadata": {},
   "source": [
    "Our robot will evolve on a map with a white background on which Aruco markers will be placed to delimit the space in which the robot will evolve as well as to calibrate the camera and detect the start and goal points.\n",
    "\n",
    "We created an example map that we will follow throughout this report :\n",
    "<br>\n",
    "\n",
    "<img src=\"images/Start_Goal.jpg\"/>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa682e20",
   "metadata": {},
   "source": [
    "### **Libraries required for programs to work properly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e46aa4f-d634-4455-b3d7-7080cd8d87ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import asyncio\n",
    "import cv2\n",
    "import numpy as np\n",
    "from heapq import heappush, heappop\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c183a6",
   "metadata": {},
   "source": [
    "## **Vision Part**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b7d185",
   "metadata": {},
   "source": [
    "#### **Constants used throughout the vision part**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aee5559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size to be adjusted according to the size of the environment\n",
    "Size_img_x = 600 # Size image height\n",
    "Size_img_y = 1000 # Size image width\n",
    "\n",
    "Camera = 0 # Camera used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca5d4c7",
   "metadata": {},
   "source": [
    "**WARNING** It is important to press the 'L' key (lower case) if the code uses vision capture as there is a risk of killing the kernel. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8058a2a",
   "metadata": {},
   "source": [
    "Initially, we decided to base our vision system on color markers with different shapes. However, the problem was that the detection reliability was not optimal and was highly sensitive to the room's lighting conditions. Therefore, we chose to switch to using Aruco markers, which are much easier to detect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e50b65c",
   "metadata": {},
   "source": [
    "#### **Creation Aruco marks**\n",
    "\n",
    "We decided to use the Aruco marks to detect the limitations of our environment, the start and finish points and the real-time information from our robot (position and angle of rotation). Initially, we did all the detection based on the detection of coloured dots, but we noticed that this type of detection was much less reliable than detection with aruco markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7287a73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_aruco_marker(marker_id, side_pixels, output_filename):\n",
    "    \"\"\"\n",
    "    Function to create aruco markers \n",
    "    \n",
    "    Input : marker_id -> Marker ID\n",
    "          : side_pixels -> Pixel size of the marker\n",
    "          : output_filename -> Where the image will be saved\n",
    "    Output : An image file containing the aruco marker created\n",
    "    \"\"\"\n",
    "    # Load the Aruco dictionary to identify Aruco markers\n",
    "    aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_50)\n",
    "    # Creation of the image\n",
    "    marker_image = np.zeros((side_pixels, side_pixels), dtype=np.uint8)\n",
    "    # Generation of the image corresponding to the desired id\n",
    "    cv2.aruco.generateImageMarker(aruco_dict, marker_id, side_pixels, marker_image, 1)\n",
    "\n",
    "    # Save Image\n",
    "    cv2.imwrite(output_filename, marker_image)\n",
    "    print(f\"Aruco marker (ID={marker_id}) saved in {output_filename}\")\n",
    "\n",
    "# Choose the id, marker size and image name\n",
    "generate_aruco_marker(marker_id=6, side_pixels=200, output_filename=\"aruco_marker_6.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f24f140",
   "metadata": {},
   "source": [
    "### **Orthogonal Projection :**\n",
    "\n",
    "Before starting the vision for our robot, we need to define the environment in which it will navigate. Our camera needs to be able to detect the boundaries of the environment, and be able to see the map perfectly from above, to minimize errors. So we decided to make an orthogonal projection using homography to get a map seen from above.\n",
    "\n",
    "We have defined the Aruco marker to delimit our environment: \n",
    "- ID 3: Top left corner\n",
    "- ID 4 : Top right corner\n",
    "- ID 5 : Bottom left corner\n",
    "- ID 6 : Bottom right corner\n",
    "\n",
    "We defined the environment in a image with these coordinates :\n",
    "<br>\n",
    "\n",
    "<img src=\"images/Image_definition.jpg\"/>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21804b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_aruco_corners(image):\n",
    "    \"\"\"\n",
    "    Function to detect the limits of the environment in which the robot will navigate by 4 Aruco markers from ID 3 to 6 \n",
    "    (ID 3 : top left corner, ID 4 : top right corner, ID 5 : bottom left corner, ID 6 : bottom right corner).\n",
    "\n",
    "    Input : image -> Input image (from camera)\n",
    "    Output : Corner dictionary for specified IDs\n",
    "    \"\"\"\n",
    "    # Load the Aruco dictionary and detector\n",
    "    aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_50)\n",
    "    parameters = cv2.aruco.DetectorParameters()\n",
    "    detector = cv2.aruco.ArucoDetector(aruco_dict, parameters)\n",
    "\n",
    "    # Detect markers\n",
    "    corners, ids, _ = detector.detectMarkers(image)\n",
    "\n",
    "    # Filter for markers with IDs 3, 4, 5, and 6\n",
    "    required_ids = {3, 4, 5, 6}\n",
    "    if ids is not None:\n",
    "        # Map detected IDs to their corners if the ID is in the required list\n",
    "        id_to_corners = {id[0]: corner[0] for id, corner in zip(ids, corners) if id[0] in required_ids}\n",
    "        if id_to_corners:\n",
    "            return id_to_corners\n",
    "        else:\n",
    "            print(\"None of the required markers (3, 4, 5, 6) were detected.\")\n",
    "    else:\n",
    "        print(\"No Aruco marker detected.\")\n",
    "\n",
    "    return None\n",
    "\n",
    "def calculate_homography(id_to_corners, output_size=(Size_img_y, Size_img_x)):\n",
    "    \"\"\"\n",
    "    Function to calculate the homography matrix, estimating missing corners if some markers are occluded.\n",
    "\n",
    "    Input: id_to_corners -> Dictionary of detected marker corners\n",
    "           output_size -> Size of the projected image\n",
    "    Output: homography_matrix -> Homography matrix\n",
    "    \"\"\"\n",
    "    # Initialize corner points (corners of the environment)\n",
    "    corner_points = np.zeros((4, 2), dtype=np.float32)\n",
    "    detected_ids = list(id_to_corners.keys())\n",
    "    \n",
    "    # Estimate missing corners\n",
    "    if 3 in detected_ids:  # ID 3 : top left corner\n",
    "        corner_points[0] = id_to_corners[3][0]\n",
    "    else:\n",
    "        # Estimate : Opposite corner to the bottom-right corner (ID 6)\n",
    "        if 6 in detected_ids and 4 in detected_ids:\n",
    "            corner_points[0] = id_to_corners[4][1] - (id_to_corners[6][2] - id_to_corners[4][1])\n",
    "        else:\n",
    "            raise ValueError(\"Unable to estimate the top-left corner\")\n",
    "    \n",
    "    if 4 in detected_ids:  # ID 4 : top right corner\n",
    "        corner_points[1] = id_to_corners[4][1]\n",
    "    else:\n",
    "        # Estimate : Opposite corner to the bottom-left corner (ID 5)\n",
    "        if 3 in detected_ids and 5 in detected_ids:\n",
    "            corner_points[1] = id_to_corners[3][0] + (id_to_corners[5][3] - id_to_corners[3][0])\n",
    "        else:\n",
    "            raise ValueError(\"Unable to estimate the top-right corner\")\n",
    "    \n",
    "    if 6 in detected_ids:  # ID 6 : bottom right corner\n",
    "        corner_points[2] = id_to_corners[6][2]\n",
    "    else:\n",
    "        # Estimate : Opposite corner to the top-left corner (ID 3)\n",
    "        if 5 in detected_ids and 4 in detected_ids:\n",
    "            corner_points[2] = id_to_corners[5][3] + (id_to_corners[4][1] - id_to_corners[5][3])\n",
    "        else:\n",
    "            raise ValueError(\"Unable to estimate the bottom-right corner\")\n",
    "    \n",
    "    if 5 in detected_ids:  # ID 5 : bottom left corner\n",
    "        corner_points[3] = id_to_corners[5][3]\n",
    "    else:\n",
    "        # Estimate : Opposite corner to the top-right corner (ID 4)\n",
    "        if 3 in detected_ids and 6 in detected_ids:\n",
    "            corner_points[3] = id_to_corners[3][0] + (id_to_corners[6][2] - id_to_corners[3][0])\n",
    "        else:\n",
    "            raise ValueError(\"Unable to estimate the bottom-left corner\")\n",
    "    \n",
    "    # Target points (rectangle in the projected space)\n",
    "    dst_points = np.array([\n",
    "        [0, 0],  # Top-left of the output\n",
    "        [output_size[0] - 1, 0],  # Top-right of the output\n",
    "        [output_size[0] - 1, output_size[1] - 1],  # Bottom-right of the output\n",
    "        [0, output_size[1] - 1]  # Bottom-left of the output\n",
    "    ], dtype=np.float32)\n",
    "    \n",
    "    # Calculate the homography matrix\n",
    "    homography_matrix, _ = cv2.findHomography(corner_points, dst_points)\n",
    "    return homography_matrix\n",
    "\n",
    "def apply_homography(image, homography_matrix, output_size):\n",
    "    \"\"\"\n",
    "    Function to apply the homography matrix to obtain an orthogonal projection of the environment\n",
    "\n",
    "    Input : image -> Input image (from camera)\n",
    "          : homography_matrix -> Homography matrix\n",
    "          : output_size -> Size of projected image\n",
    "    Output : projected_image -> Projected image\n",
    "    \"\"\"\n",
    "    projected_image = cv2.warpPerspective(image, homography_matrix, output_size)\n",
    "    return projected_image\n",
    "\n",
    "cap = cv2.VideoCapture(Camera)\n",
    "\n",
    "while True:\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Detection of Aruco markers for map delimitation\n",
    "    id_to_corners = detect_aruco_corners(frame)\n",
    "\n",
    "    # If we detected Aruco markers for map delimitation\n",
    "    if id_to_corners :\n",
    "        try:\n",
    "            # Environment projection operation\n",
    "            homography_matrix = calculate_homography(id_to_corners)\n",
    "            output_size = (Size_img_y, Size_img_x)\n",
    "            projected_frame = apply_homography(frame, homography_matrix, output_size)\n",
    "            # Afficher l'image\n",
    "            cv2.imshow(\"Calibration with Aruco\", projected_frame)\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(f\"Error with projected image : {e}\")\n",
    "\n",
    "    # Stop video capture by pressing l\n",
    "    if cv2.waitKey(1) & 0xFF == ord('l'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# We saved the homography matrix for all the others programs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbd591e",
   "metadata": {},
   "source": [
    "We have this result (we've masked the elements pasted on our map): \n",
    "<br>\n",
    "\n",
    "<img src=\"images/Homography.jpg\"/>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d3396d",
   "metadata": {},
   "source": [
    "### **Calibration camera :**\n",
    "\n",
    "Now we need to calibrate our camera so that we know the ratio between a pixel and the actual distance in the environment, in order to guide our robot as effectively as possible.\n",
    "\n",
    "We therefore used an aruco marker (ID = 2) of known size (90 mm square) to perform our calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5c91d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_camera_with_aruco(img, marker_size_mm=90):\n",
    "    \"\"\"\n",
    "    Function to calibrate camera (pixel/mm). We calibrate the camera using a 9cm aruco marker to find out the actual size of the environment.\n",
    "    \n",
    "    Input : img -> Input image (projected image) \n",
    "          : marker_size_mm = 90mm -> Size of Aruco marker\n",
    "    Output : pixel_to_mm -> Distance from the environment (mm) for a pixel in the image\n",
    "    \"\"\"\n",
    "    # Loading the Aruco marker library\n",
    "    aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_50)\n",
    "    parameters = cv2.aruco.DetectorParameters()\n",
    "    detector = cv2.aruco.ArucoDetector(aruco_dict, parameters)\n",
    "\n",
    "    # Detecting the Aruco marker\n",
    "    corners, ids, _ = detector.detectMarkers(img)\n",
    "\n",
    "    if ids is not None:\n",
    "        for corner, marker_id in zip(corners, ids):\n",
    "            if marker_id[0] == 2: # Only the Id = 2 marker is used for calibration\n",
    "                # Extraction of marker coordinates\n",
    "                points = corner[0]\n",
    "\n",
    "                # Calculate dimensions in pixels\n",
    "                width_px = np.linalg.norm(points[1] - points[2])  # Size y\n",
    "                height_px = np.linalg.norm(points[0] - points[1])  # Size x\n",
    "\n",
    "                # Average dimensions\n",
    "                avg_size_px = (width_px + height_px) / 2\n",
    "\n",
    "                # Calculation of the actual distance from the environment (mm) for a pixel in the image\n",
    "                pixel_to_mm = marker_size_mm / avg_size_px\n",
    "\n",
    "                return pixel_to_mm\n",
    "\n",
    "    return None\n",
    "\n",
    "# Video Capture\n",
    "cap = cv2.VideoCapture(Camera)\n",
    "\n",
    "delay_between_calibrations = 5 # Time beetween 2 measurements (in seconds)\n",
    "last_calibration_time = 0 # Initial time (in seconds)\n",
    "\n",
    "while True:\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    current_time = time.time() \n",
    "    output_size = (Size_img_y, Size_img_x)\n",
    "    projected_frame = apply_homography(frame, homography_matrix, output_size)\n",
    "    # Calculation of the actual distance from the environment (mm) for a pixel in the image\n",
    "    # Measurements are taken every 5 seconds to avoid saturating the output\n",
    "    if current_time - last_calibration_time >= delay_between_calibrations:\n",
    "        pixel_to_mm = calibrate_camera_with_aruco(projected_frame, marker_size_mm=90)\n",
    "        last_calibration_time = current_time\n",
    "        # If we have a pixel/mm value\n",
    "        if pixel_to_mm:\n",
    "            print(f\"Calibration : mm/pixel : {pixel_to_mm:.4f}\")\n",
    "    # Afficher l'image\n",
    "    cv2.imshow(\"Calibration with Aruco\", projected_frame)\n",
    "\n",
    "    # Stop video capture by pressing l\n",
    "    if cv2.waitKey(1) & 0xFF == ord('l'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebbbcba",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<img src=\"images/Calibration.jpg\"/>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7095ef93",
   "metadata": {},
   "source": [
    "### **Obstacle detection :** \n",
    "\n",
    "Now we need to detect obstacles in our environment to create areas where the robot cannot go. We assume that : \n",
    "- the obstacles are black\n",
    "- the aruco markers that delimit the map will be considered as obstacles\n",
    "\n",
    "Next, we will perform morphological transformations on our image (dilation, erosion) in order to remove potential noise and to enlarge the obstacles by at least half the width of the robot so that the robot does not collide with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "80faf085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obstacles_detection(img, threshold, pixel_to_mm):\n",
    "    \"\"\"\n",
    "    Function to detect obstacles in the environment so that they can be taken into account in the matrix supplied to the A* Dijkstra algorithm.\n",
    "    \n",
    "    Input : img -> Input image (projected image) \n",
    "          : threshold -> threshold to detect more or less bright objects in the image\n",
    "          : pixel_to_mm -> Distance from the environment (mm) for a pixel in the image\n",
    "    Output : erode_img -> Image projected after the various morphological transformations\n",
    "           : matrix -> Matrix of the image to be supplied to the A* dijkstra algorithm\n",
    "    \"\"\"\n",
    "    # Conversion Grayscale\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Threshold to detect dark obstacles (we can adjust the threshold to detect more or less obstacles -> it depends on the darker or lighter colour of the obstacles)\n",
    "    _, binary_image = cv2.threshold(gray_img, threshold, 255, cv2.THRESH_BINARY)\n",
    "    # We're adding pixels to the edges of the image to indicate obstacles so that my robot doesn't leave the environment\n",
    "    binary_image = cv2.copyMakeBorder(binary_image, 1, 1, 1, 1, cv2.BORDER_CONSTANT, value=0)\n",
    "\n",
    "    # We apply a structuring element which is a circle with a diameter of 5 pixels to remove noise\n",
    "    denoising_kernel_size = 5\n",
    "    denoising_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (denoising_kernel_size, denoising_kernel_size))\n",
    "    # We apply a structuring element which is a circle with a diameter of half width of robot transform in pixels to enlarge obstacles to the width of the robot\n",
    "    kernel_size = int(85 / pixel_to_mm)  # (65 mm (half width of robot) + 20 mm (margin)) / Ratio (mm/pixel)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))\n",
    "\n",
    "    # Apply closing (to remove noise) and erosion (to adapt ostacles to width of robot)\n",
    "    closing_img = cv2.morphologyEx(binary_image, cv2.MORPH_CLOSE, denoising_kernel)\n",
    "    erode_img = cv2.erode(closing_img, kernel, iterations=1)\n",
    "    # We remove the pixels added for obstacles on the edges of the image\n",
    "    erode_img = erode_img[1:-1, 1:-1]\n",
    "\n",
    "    # Creation of matrix to find the optimal path (for the A* Djikstra Algorithm)\n",
    "    matrix_init = (erode_img / 255).astype(int) \n",
    "    # We replace the 0‘s with -1’s (to detect obstacles) and the 1‘s with 0’s (to detect the free environment).\n",
    "    matrix = np.where(matrix_init == 0, -1, 0)\n",
    "\n",
    "    return erode_img, matrix\n",
    "\n",
    "# Video Capture\n",
    "cap = cv2.VideoCapture(Camera)\n",
    "\n",
    "# Capturing an image\n",
    "success, frame = cap.read()\n",
    "\n",
    "output_size = (Size_img_y, Size_img_x)\n",
    "projected_frame = apply_homography(frame, homography_matrix, output_size)\n",
    "\n",
    "# Obstacles detection\n",
    "erode_img, matrix = obstacles_detection(projected_frame, 60, pixel_to_mm)\n",
    "cv2.imshow(\"Binary image\", erode_img)\n",
    "\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30623f1d",
   "metadata": {},
   "source": [
    "Before Morphological Transfomation :\n",
    "<br>\n",
    "\n",
    "<img src=\"images/Map_with_obstacles.jpg\"/>\n",
    "\n",
    "<br>\n",
    "After Morphological Transfomation : \n",
    "<br>\n",
    "\n",
    "<img src=\"images/Binary_map.jpg\"/>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68a73b9",
   "metadata": {},
   "source": [
    "### **Start and Goal Point Detection :** \n",
    "\n",
    "We now need to detect the robot's start and end points. To do this, we'll use the aruco markers again, and more specifically the ID = 0 marker for the start point and the ID = 1 marker for the end point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b8a257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_start_and_goal_aruco(img):\n",
    "    \"\"\"\n",
    "    Function to detect start and goal point in the environment\n",
    "    \n",
    "    Input : img -> Input image (projected image) \n",
    "    Output : img -> Image with markers indications\n",
    "           : start_position -> Coordinates of start point (pixels)\n",
    "           : goal_position -> Coordinates of goal point (pixels)\n",
    "    \"\"\"\n",
    "    # Load the Aruco dictionary to identify Aruco markers\n",
    "    aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_50)\n",
    "    parameters = cv2.aruco.DetectorParameters()\n",
    "    detector = cv2.aruco.ArucoDetector(aruco_dict, parameters)\n",
    "    \n",
    "    # Detection Aruco Marks\n",
    "    corners, ids, _ = detector.detectMarkers(img)\n",
    "    \n",
    "    start_position = None\n",
    "    goal_position = None\n",
    "    \n",
    "    # Detection all Aruco Marks\n",
    "    if ids is not None:\n",
    "        for i, marker_id in enumerate(ids.flatten()):\n",
    "            x_center = int(corners[i][0][:, 0].mean())\n",
    "            y_center = int(corners[i][0][:, 1].mean())\n",
    "\n",
    "            # Detection Start point (ID 0)\n",
    "            if marker_id == 0:\n",
    "                start_position = (y_center, x_center)\n",
    "                cv2.circle(img, (x_center, y_center), 5, (255, 0, 0), -1)\n",
    "                cv2.putText(img, \"Start\", (x_center - 20, y_center - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)\n",
    "            \n",
    "            # Detection Goal point (ID 1)\n",
    "            elif marker_id == 1:\n",
    "                goal_position = (y_center, x_center)\n",
    "                cv2.circle(img, (x_center, y_center), 5, (0, 255, 0), -1)\n",
    "                cv2.putText(img, \"Goal\", (x_center - 20, y_center - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "    \n",
    "    return img, start_position, goal_position\n",
    "\n",
    "# Video capture\n",
    "cap = cv2.VideoCapture(Camera)\n",
    "\n",
    "delay_between_measurements = 5 # Time beetween 2 measurements (in seconds)\n",
    "last_measurement_time = 0 # Initial time (in seconds)\n",
    "\n",
    "while True:\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "    \n",
    "    # Detection of Aruco markers for map delimitation\n",
    "    current_time = time.time() \n",
    "        \n",
    "    output_size = (Size_img_y, Size_img_x) \n",
    "    projected_frame = apply_homography(frame, homography_matrix, output_size)\n",
    "\n",
    "    # Detection start and goal Aruco marks\n",
    "    processed_img, start_position, goal_position = detect_start_and_goal_aruco(projected_frame)\n",
    "    \n",
    "    if current_time - last_measurement_time >= delay_between_measurements:\n",
    "        last_measurement_time = current_time\n",
    "        # Print position\n",
    "        print(\"Start position :\", start_position)\n",
    "        print(\"Goal position :\", goal_position)\n",
    "\n",
    "    # Print image\n",
    "    cv2.imshow(\"Detection ArUco Start and Goal\", processed_img)\n",
    "    \n",
    "    # Stop video capture by pressing l\n",
    "    if cv2.waitKey(1) & 0xFF == ord('l'):\n",
    "        break\n",
    "\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0851d272",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<img src=\"images/Start_Goal.jpg\"/>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad0485f",
   "metadata": {},
   "source": [
    "### **Detection position and angle of rotation of the robot :** \n",
    "\n",
    "Now we need to detect the robot's position and angle of rotation in real time in order to provide information to the Kalman filter. Using the aruco marker ID = 2, we can extract this information in real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6290f786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_angle(angle):\n",
    "    \"\"\"\n",
    "    Function to normalize an angle to the interval [-π, π].\n",
    "    \n",
    "    Input : angle -> Angle to be normalized (in radians)\n",
    "    Output : Normalized angle within the range [-π, π] (in radians)\n",
    "    \"\"\"\n",
    "    return (angle + np.pi) % (2 * np.pi) - np.pi\n",
    "\n",
    "def detect_aruco_marker(img, target_id=2):\n",
    "    \"\"\"\n",
    "    Function to detect the position and angle of rotation of the Aruco ID = 2 marker positioned on the robot\n",
    "    \n",
    "    Input : img -> Input image (projected image) \n",
    "          : target_id=2 -> Id number of the Aruco marker\n",
    "    Output : image -> Image with marker indications\n",
    "           : (center_x, center_y) -> Coordinates of the robot (pixels)\n",
    "           : angle -> Angle of rotation of the robot (rad)\n",
    "    \"\"\"\n",
    "\n",
    "    aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_50)\n",
    "    parameters = cv2.aruco.DetectorParameters()\n",
    "    detector = cv2.aruco.ArucoDetector(aruco_dict, parameters)\n",
    "\n",
    "    corners, ids, _ = detector.detectMarkers(img)\n",
    "    if ids is not None:\n",
    "        for corner, marker_id in zip(corners, ids):\n",
    "            if marker_id[0] == target_id:\n",
    "                points = corner[0]\n",
    "                center_x = int(points[:, 0].mean())\n",
    "                center_y = int(points[:, 1].mean())\n",
    "                # Compute angle then normalization between [-π, π]\n",
    "                vector = points[1] - points[0]\n",
    "                angle = np.atan2(vector[1], vector[0])\n",
    "                return img, (center_x, center_y), wrap_angle(angle)\n",
    "    return img, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d4f329",
   "metadata": {},
   "source": [
    "Our vision is now initialized and we have a clear knowledge of the environment in which the robot will evolve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebe20c7",
   "metadata": {},
   "source": [
    "## **Global Navigation Part**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a36b232",
   "metadata": {},
   "source": [
    "### **Find the Optimal Path : A * Dijkstra Algorithm**\n",
    "\n",
    "To find the optimal path, we decided to re-use the algorithm we had seen in class and in the exercise: the A* dijkstra algorithm. We modified it and the heuristic function so that it could take diagonal displacements into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c34f329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heuristic(a, b):\n",
    "    \"\"\"\n",
    "    Function to calculate the Euclidean distance between two points for the heuristic function.\n",
    "    \n",
    "    Input : a -> Coordinates of the first point \n",
    "          : b -> Coordinates of the second point \n",
    "    Output : Euclidean distance between the two points \n",
    "    \"\"\"\n",
    "    # Euclidean distance\n",
    "    return np.sqrt((b[0] - a[0]) ** 2 + (b[1] - a[1]) ** 2)\n",
    "\n",
    "def a_star_search(map_grid, start, goal):\n",
    "    \"\"\"\n",
    "    Function to perform A* search algorithm to find the optimal path from a start point to a goal point.\n",
    "    \n",
    "    Input : map_grid -> 2D array representing the environment (-1 for obstacles, 0 for free space)\n",
    "          : start -> Starting coordinates from vision\n",
    "          : goal -> Goal coordinates from vision\n",
    "    Output : path -> List of coordinates representing the optimal path \n",
    "           : explored -> Set of nodes that were explored during the search \n",
    "           : operation_count -> Number of operations performed during the search \n",
    "    \"\"\"\n",
    "    # Initialize the open set as a priority queue and add the start node\n",
    "    open_set = []\n",
    "    heappush(open_set, (heuristic(start, goal), 0, start))  # (f_cost, g_cost, position)\n",
    "\n",
    "    # Initialize the came_from dictionary\n",
    "    came_from = {}\n",
    "    # Initialize g_costs dictionary with default value of infinity and set g_costs[start] = 0\n",
    "    g_costs = {start: 0}\n",
    "    # Initialize the explored set\n",
    "    explored = set()\n",
    "    operation_count = 0\n",
    "\n",
    "    while open_set:\n",
    "        # Pop the node with the lowest f_cost from the open set\n",
    "        current_f_cost, current_g_cost, current_pos = heappop(open_set)\n",
    "\n",
    "        # Add the current node to the explored set\n",
    "        explored.add(current_pos)\n",
    "\n",
    "        # For directly reconstruct path\n",
    "        if current_pos == goal:\n",
    "            break\n",
    "\n",
    "        # Get the neighbors of the current node (up, down, left, right)\n",
    "        neighbors = [\n",
    "            (current_pos[0] - 1, current_pos[1]),  # Up\n",
    "            (current_pos[0] + 1, current_pos[1]),  # Down\n",
    "            (current_pos[0], current_pos[1] - 1),  # Left\n",
    "            (current_pos[0], current_pos[1] + 1),  # Right\n",
    "            (current_pos[0]-1, current_pos[1]-1),  # Up-left\n",
    "            (current_pos[0]-1, current_pos[1]+1),  # Up-right\n",
    "            (current_pos[0]+1, current_pos[1]+1),  # Down-right\n",
    "            (current_pos[0]+1, current_pos[1]-1)   # Down-left\n",
    "        ]\n",
    "\n",
    "        for neighbor in neighbors:\n",
    "            # Check if neighbor is within bounds and not an obstacle\n",
    "            if (0 <= neighbor[0] < map_grid.shape[0]) and (0 <= neighbor[1] < map_grid.shape[1]):\n",
    "                if map_grid[neighbor[0], neighbor[1]] != -1 and neighbor not in explored:\n",
    "                    # Calculate tentative_g_cost\n",
    "                    # if diagonal --> cost = sqrt(2)\n",
    "                    if (neighbor==[current_pos[0]-1, current_pos[1]-1] or neighbor==[current_pos[0]-1, current_pos[1]+1] or neighbor==[current_pos[0]+1, current_pos[1]+1] or neighbor==[current_pos[0]+1, current_pos[1]-1]):\n",
    "                        tentative_g_cost = current_g_cost + np.sqrt(2) + map_grid[neighbor[0], neighbor[1]]\n",
    "                    # else --> cost = 1\n",
    "                    else :\n",
    "                        tentative_g_cost = current_g_cost + 1 + map_grid[neighbor[0], neighbor[1]]\n",
    "\n",
    "                    # If this path to neighbor is better than any previous one\n",
    "                    if neighbor not in g_costs or tentative_g_cost < g_costs[neighbor]:\n",
    "                        # Update came_from, g_costs, and f_cost\n",
    "                        came_from[neighbor] = current_pos\n",
    "                        g_costs[neighbor] = tentative_g_cost\n",
    "                        f_cost = tentative_g_cost + heuristic(neighbor, goal)\n",
    "\n",
    "                        # Add neighbor to open set\n",
    "                        heappush(open_set, (f_cost, tentative_g_cost, neighbor))\n",
    "                        operation_count += 1\n",
    "\n",
    "    # Reconstruct path\n",
    "    if current_pos == goal:\n",
    "        path = []\n",
    "        while current_pos in came_from:\n",
    "            path.append(current_pos)\n",
    "            current_pos = came_from[current_pos]\n",
    "        path.append(start)\n",
    "        return path[::-1], explored,operation_count\n",
    "    else:\n",
    "        # If we reach here, no path was found\n",
    "        return None, explored,operation_count\n",
    "\n",
    "def display_map(map_grid, path, start, goal, explored):\n",
    "    \"\"\"\n",
    "    Function to visualize the map grid, the explored nodes, and the optimal path.\n",
    "    \n",
    "    Input : map_grid -> 2D array representing the environment (-1 for obstacles, 0 for free space)\n",
    "          : path -> List of coordinates representing the optimal path \n",
    "          : start -> Starting coordinates from vision \n",
    "          : goal -> Goal coordinates from vision\n",
    "          : explored -> Set of nodes explored during the search \n",
    "    \"\"\"\n",
    "    cmap = ListedColormap(['white', 'black', 'blue', 'green', 'red', 'grey'])\n",
    "    map_display = np.zeros_like(map_grid, dtype=object)\n",
    "\n",
    "    # Assign colors based on the map grid values\n",
    "    map_display[map_grid == -1] = 'black'  # Obstacles\n",
    "    map_display[map_grid == 0] = 'white'   # Free space\n",
    "\n",
    "    for position in explored:\n",
    "        if map_display[tuple(position)] == 'white':\n",
    "            map_display[tuple(position)] = 'grey'  # Explored cells\n",
    "\n",
    "    # Visualize the path\n",
    "    for position in path:\n",
    "        if map_display[position[0], position[1]] in ['white', 'grey']:\n",
    "            map_display[position[0], position[1]] = 'blue'  # Path\n",
    "\n",
    "    map_display[start[0], start[1]] = 'green'  # Start\n",
    "    map_display[goal[0], goal[1]] = 'red'      # Goal\n",
    "\n",
    "    # Convert color names to numbers for plotting\n",
    "    color_mapping = {'white': 0, 'black': 1, 'blue': 2, 'green': 3, 'red': 4, 'grey': 5}\n",
    "    map_numeric_display = np.vectorize(color_mapping.get)(map_display)\n",
    "    fig, ax = plt.subplots(figsize=(72, 128))\n",
    "    ax.imshow(map_numeric_display, cmap=cmap)\n",
    "    ax.set_xticks(np.arange(-0.5, map_grid.shape[1], 1), minor=True)\n",
    "    ax.set_yticks(np.arange(-0.5, map_grid.shape[0], 1), minor=True)\n",
    "    ax.grid(which='minor', color='gray', linestyle='-', linewidth=0.5)\n",
    "    ax.tick_params(which='both', bottom=False, left=False, labelbottom=False, labelleft=False)\n",
    "    ax.set_title('a_star_search Visualization')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def classify_segments_as_vectors(path):\n",
    "    \"\"\"\n",
    "    Segments the path into continuous vectors based on direction (horizontal, vertical, diagonal).\n",
    "    \n",
    "    Input : path -> List of coordinates representing the optimal path \n",
    "    Output : vectors -> List of tuples containing the direction, start, and end points of each segment [(direction, start_point, end_point), ...]\n",
    "    \"\"\"\n",
    "    if len(path) < 2:\n",
    "        return []\n",
    "\n",
    "    vectors = [] \n",
    "    start_point = path[0] \n",
    "\n",
    "    # Identify direction beween 2 points\n",
    "    def get_direction(p1, p2):\n",
    "        dx = p2[0] - p1[0]\n",
    "        dy = p2[1] - p1[1]\n",
    "        if dx == 0:  # Horizontal\n",
    "            return \"horizontal\"\n",
    "        elif dy == 0:  # Vertical\n",
    "            return \"vertical\"\n",
    "        elif abs(dx) == abs(dy):  # Diagonal\n",
    "            return \"diagonal\"\n",
    "        else:\n",
    "            return \"other\"\n",
    "\n",
    "    # Identify continous segment\n",
    "    current_direction = get_direction(path[0], path[1])\n",
    "\n",
    "    for i in range(1, len(path)):\n",
    "        direction = get_direction(path[i - 1], path[i])\n",
    "        if direction != current_direction:\n",
    "            # End of the actual segment\n",
    "            vectors.append((current_direction, start_point, path[i - 1]))\n",
    "            # New segment\n",
    "            start_point = path[i - 1]\n",
    "            current_direction = direction\n",
    "\n",
    "    # Add the last segment\n",
    "    vectors.append((current_direction, start_point, path[-1]))\n",
    "\n",
    "    return vectors\n",
    "\n",
    "def A_Dijkstra(Map, SearchStart, SearchGoal):\n",
    "    \"\"\"\n",
    "    Function to extract the data needed to control the robot to reach the goal point\n",
    "    \n",
    "    Input : Map -> Matrix of our environment (-1 for obstacles and 0 for clear path)\n",
    "          : SearchStart -> Start point coordinates\n",
    "          :  SearchGoal -> Goal point coordinates\n",
    "    Output : start_points -> Start points for each vector in the trajectory\n",
    "           : goal_points -> Goal points for each vector in the trajectory\n",
    "           : trajectory -> Set of trajectory vectors\n",
    "    \"\"\"\n",
    "    path, explored, operation_count = a_star_search(Map, SearchStart, SearchGoal)\n",
    "    # Display the result\n",
    "    if path:\n",
    "        print(\"Path found:\")\n",
    "        print(f\"Number of operations: {operation_count}\")\n",
    "        display_map(Map, path, SearchStart, SearchGoal, explored)\n",
    "    else:\n",
    "        print(\"No path found.\")\n",
    "\n",
    "    vectors = classify_segments_as_vectors(path)\n",
    "    start_points = []\n",
    "    goal_points = []\n",
    "    trajectory = []\n",
    "\n",
    "    # Print results\n",
    "    for direction, start, end in vectors:\n",
    "        print(f\"Direction : {direction}, Début (pixels) : {start}, Fin (pixels) : {end} \")\n",
    "        print(\"Début (mm): (\" + str(start[0]*pixel_to_mm)+\";\" + str(start[1]*pixel_to_mm) + \"), Fin (mm) : (\" + str(end[0]*pixel_to_mm) + \";\" + str(end[1]*pixel_to_mm) + \")\")\n",
    "        print(\"Vector (pixels) : (\" + str(end[0]-start[0]) + \", \" + str(end[1]-start[1]) + \")\")\n",
    "        print(\"Vector (mm) : (\" + str((end[0]-start[0])*pixel_to_mm) + \", \" + str((end[1]-start[1])*pixel_to_mm) + \")\")\n",
    "        \n",
    "        # Table containing the values of start points of each vector (according to x and y) which allow the robot to be guided to the global goal point\n",
    "        start_points = start_points + [[start[0]*pixel_to_mm, start[1]*pixel_to_mm]]\n",
    "        # Table containing the values of goal points of each vector (according to y and x)\n",
    "        goal_points = goal_points + [[end[1]*pixel_to_mm, end[0]*pixel_to_mm]]\n",
    "        # Table containing the values of vectors according to x and y\n",
    "        trajectory = trajectory + [[(end[0]-start[0])*pixel_to_mm, (end[1]-start[1])*pixel_to_mm]]\n",
    "\n",
    "    return start_points, goal_points, trajectory\n",
    "\n",
    "# Application of the A* Dijkstra algorithm\n",
    "start_points, goal_points, trajectory = A_Dijkstra(matrix, start_position, goal_position)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b94059",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<img src=\"images/Dijkstra.png\"/>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858b02ba",
   "metadata": {},
   "source": [
    "We now have our Global Navigation and our vision ready."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771bf3d6",
   "metadata": {},
   "source": [
    "## **Motion Control Part**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fab02f4",
   "metadata": {},
   "source": [
    "To control our robot and ensure it reaches the goal point, we implemented Astolfi's Differential Drive Motion Control system. This approach computes velocity commands based on the robot's position and orientation relative to the target. Below is a breakdown of the theory and implementation.\n",
    "\n",
    "### Kinematic Model\n",
    "The kinematic model relates the robot's velocity ($v$, $\\omega$) to its position ($\\rho$) and angles ($\\alpha$, $\\beta$) over time.\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; align-items: center; gap: 15px;\">\n",
    "    <figure style=\"text-align: center;\">\n",
    "        <img src=\"images/Motion_1.jpg\" alt=\"Kinematic Model\" style=\"max-width: 40%;\"/>\n",
    "        <figcaption>Kinematic Model: Relation between velocity, position, and angles.</figcaption>\n",
    "    </figure>\n",
    "    <figure style=\"text-align: center;\">\n",
    "        <img src=\"images/Motion_2.jpg\" alt=\"Control Law\" style=\"max-width: 100%;\"/>\n",
    "        <figcaption>Control Law: Computes velocity commands.</figcaption>\n",
    "    </figure>\n",
    "</div>\n",
    "\n",
    "### Control Law\n",
    "This law computes the robot’s linear ($v$) and angular ($\\omega$) velocities based on feedback control gains and the robot's state.\n",
    "\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <figure style=\"text-align: center;\">\n",
    "        <img src=\"images/Motion_3.jpg\" alt=\"Closed-Loop System\" style=\"max-width: 80%;\"/>\n",
    "        <figcaption>Closed-Loop System: Ensures convergence to the goal.</figcaption>\n",
    "    </figure>\n",
    "</div>\n",
    "\n",
    "### Stability Conditions\n",
    "The system is exponentially stable under the following conditions (proven using Lyapunov theory):\n",
    "- $k_\\rho > 0$\n",
    "- $k_\\alpha < 0$\n",
    "- $k_\\beta < 0$\n",
    "- $k_\\rho > |k_\\alpha \\cdot k_\\beta|$\n",
    "\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <figure style=\"text-align: center;\">\n",
    "        <img src=\"images/Motion_4.jpg\" alt=\"Stability Conditions\" style=\"max-width: 60%;\"/>\n",
    "        <figcaption>Stability Conditions: Exponential stability of the system.</figcaption>\n",
    "    </figure>\n",
    "</div>\n",
    "\n",
    "### Final System Overview\n",
    "\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <figure style=\"text-align: center;\">\n",
    "        <img src=\"images/Motion_5.jpg\" alt=\"Final System\" style=\"max-width: 85%;\"/>\n",
    "        <figcaption>Final System: Integrated control setup overview.</figcaption>\n",
    "    </figure>\n",
    "</div>\n",
    "\n",
    "### Implementation in Code\n",
    "The astolfi_control function calculates motor speed commands using the kinematic model. It computes:\n",
    "- *$\\rho$*: Distance to the target.  \n",
    "- *$\\alpha$*: Angle difference between the robot's heading and the goal.  \n",
    "- *$\\beta$*: Remaining orientation difference to align with the goal.\n",
    "\n",
    "With tuned control gains ($k_\\rho = 0.5$, $k_\\alpha = 3$, $k_\\beta = -0.5$), we calculate the robot’s velocities and convert them into motor commands for the left and right wheels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "03b7b059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def astolfi_control(robot_position, robot_orientation, goal_position):\n",
    "    \"\"\"\n",
    "    Implements Astolfi's controller to compute the speed commands for the robot to move towards a goal point.\n",
    "    \n",
    "    Input : robot_position -> Current position of the robot as (x, y) coordinates \n",
    "          : robot_orientation -> Current orientation of the robot (in radians)\n",
    "          : goal_position -> Desired goal position as (x, y) coordinates \n",
    "    Output : spLeft -> Speed command for the left motor \n",
    "           : spRight -> Speed command for the right motor \n",
    "    \"\"\"\n",
    "    k_rho, k_alpha, k_beta = 0.3, 1.5, -0.5\n",
    "    delta_x = goal_position[0] - robot_position[0]\n",
    "    delta_y = goal_position[1] - robot_position[1]\n",
    "\n",
    "    rho = np.sqrt(delta_x**2 + delta_y**2)\n",
    "    alpha = (wrap_angle(np.atan2(delta_y, delta_x) - robot_orientation))*(-1)\n",
    "    beta = wrap_angle(-alpha - robot_orientation)\n",
    "\n",
    "    v = k_rho * rho\n",
    "    omega = k_alpha * alpha + k_beta * beta\n",
    "    spLeft = int(v - omega * 10)\n",
    "    spRight = int(v + omega * 10)\n",
    "    return spLeft, spRight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3fcb93",
   "metadata": {},
   "source": [
    "## **Local Navigation Part**\n",
    "\n",
    "The robot's navigation system is designed to handle local obstacles introduced dynamically. Using weighted proximity sensor readings, the system adjusts motor speeds to avoid collisions. Symmetrical sensor weights ensure balanced reactions to obstacles on either side, while the central sensor slows the robot for frontal obstacles. This approach enables smooth navigation in the presence of unexpected objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c69e00f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "obstSpeedGain = [4, 3, -1, -3, -4]  # Obstacle detection weights\n",
    "\n",
    "def obstacle_avoidance(prox_values):\n",
    "    \"\"\"\n",
    "    Function to compute corrections to motor speeds to avoid obstacles based on proximity sensor readings (prox.horizontal).\n",
    "    \n",
    "    Input : prox_values -> List of proximity sensor values \n",
    "    Output : spLeft -> Updated speed command for the left motor \n",
    "           : spRight -> Updated speed command for the right motor \n",
    "    \"\"\"\n",
    "    global spLeft, spRight\n",
    "    for i in range(5):\n",
    "        spLeft += prox_values[i] * obstSpeedGain[i] // 100\n",
    "        spRight += prox_values[i] * obstSpeedGain[4 - i] // 100\n",
    "    return spLeft, spRight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77505fb1",
   "metadata": {},
   "source": [
    "## **Filtering Part**\n",
    "\n",
    "The filtering part explains how the Thymio robot identifies itself on the map. Since our application requires a smooth and contunuous state estimation, and Extended Kalman Filter (EKF) is an algorithm with relativlely high computational efficiency that a more reliable pose estimation by fusing the results from our model and sensor. We utilize EKF to compute the pose of our Thymio robot as our system is non-linear and the noise is approximately Gaussian. The EKF algorithm is defined as following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5fc7e2",
   "metadata": {},
   "source": [
    "### **EKF Algortihm**\n",
    "#### Parameters for EKF\n",
    "\n",
    "1. **Wheel Base ($ b $)**: Distance between the wheels.\n",
    "   $$\n",
    "   b = 95 \\ \\text{mm}\n",
    "   $$\n",
    "\n",
    "2. **Time Step ($ \\Delta t $)**: The interval between each prediction.\n",
    "   $$\n",
    "   \\Delta t = 0.1 \\ \\text{seconds}\n",
    "   $$\n",
    "\n",
    "3. **Process Noise Covariance ($ Q $)**: (To be derived in the next section)\n",
    "\n",
    "   $$\n",
    "   Q = \\begin{bmatrix}\n",
    "   \\sigma_{\\text{x position, odometry}}^2 & 0 & 0 & 0 \\\\\n",
    "   0 & \\sigma_{\\text{y position, odometry}}^2 & 0 & 0 \\\\\n",
    "   0 & 0 & \\sigma_{\\theta\\text{, odometry}}^2 & 0 \\\\\n",
    "   0 & 0 & 0 & \\sigma_{\\text{velocity, odometry}}^2 \\\\\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "\n",
    "4. **Measurement Noise Covariance ($ R $)**: (To be derived in the next section)\n",
    "   $$\n",
    "   Q = \\begin{bmatrix}\n",
    "   \\sigma_{\\text{x position, camera}}^2 & 0 & 0 & 0 \\\\\n",
    "   0 & \\sigma_{\\text{y position, camera}}^2 & 0 & 0 \\\\\n",
    "   0 & 0 & \\sigma_{\\theta\\text{, camera}}^2 & 0 \\\\\n",
    "   0 & 0 & 0 & \\sigma_{\\text{velocity, camera}}^2 \\\\\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "   \n",
    "5. **Initial State Estimate ($ \\hat{X}_0 $)**:\n",
    "   $$\n",
    "   \\hat{X}_0 = \\begin{bmatrix}\n",
    "   x_0 \\\\\n",
    "   y_0 \\\\\n",
    "   \\theta_0 \\\\\n",
    "   v_0 \\\\\n",
    "   \\end{bmatrix} = \\begin{bmatrix}\n",
    "   0 \\\\\n",
    "   0 \\\\\n",
    "   0 \\\\\n",
    "   0 \\\\\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "6. **Initial Covariance ($ P_0 $)**:\n",
    "   $$\n",
    "   P_0 = I_4 = \\begin{bmatrix}\n",
    "   1 & 0 & 0 & 0 \\\\\n",
    "   0 & 1 & 0 & 0 \\\\\n",
    "   0 & 0 & 1 & 0 \\\\\n",
    "   0 & 0 & 0 & 1 \\\\\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "#### EKF Prediction Step\n",
    "\n",
    "##### Inputs:\n",
    "\n",
    "- Previous state estimate: $ \\hat{X}_{k-1} = \\begin{bmatrix} x_{k-1} \\\\ y_{k-1} \\\\ \\theta_{k-1} \\\\ v_{k-1} \\end{bmatrix} $\n",
    "- Previous covariance: $ P_{k-1} $\n",
    "- Left wheel velocity: $ v_l $\n",
    "- Right wheel velocity: $ v_r $\n",
    "\n",
    "##### Calculations:\n",
    "\n",
    "1. **Compute Linear and Angular Velocities**:\n",
    "\n",
    "   - **Linear Velocity ($ v_k $)**:\n",
    "     $$\n",
    "     v_k = \\frac{v_r + v_l}{2}\n",
    "     $$\n",
    "\n",
    "   - **Angular Velocity ($ \\omega_k $)**:\n",
    "     $$\n",
    "     \\omega_k = \\frac{v_r - v_l}{b}\n",
    "     $$\n",
    "\n",
    "2. **Compute the Jacobian of the Motion Model ($ F_k $)**:\n",
    "\n",
    "   $$\n",
    "   F_k = \\begin{bmatrix}\n",
    "   1 & 0 & -v_k \\Delta t \\sin(\\theta_k) & \\Delta t \\cos(\\theta_k) \\\\\n",
    "   0 & 1 & v_k \\Delta t \\cos(\\theta_k) & \\Delta t \\sin(\\theta_k) \\\\\n",
    "   0 & 0 & 1 & \\Delta t \\\\\n",
    "   0 & 0 & 0 & 1 \\\\\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   Where $ \\theta_k = \\theta_{k-1} $.\n",
    "\n",
    "3. **Predict the Next State ($ \\hat{X}_{k|k-1} $)**:\n",
    "\n",
    "   $$\n",
    "   \\hat{X}_{k|k-1} = \\begin{bmatrix}\n",
    "   x_{k|k-1} \\\\\n",
    "   y_{k|k-1} \\\\\n",
    "   \\theta_{k|k-1} \\\\\n",
    "   v_{k|k-1} \\\\\n",
    "   \\end{bmatrix} = \\begin{bmatrix}\n",
    "   x_{k-1} + v_k \\Delta t \\cos(\\theta_k) \\\\\n",
    "   y_{k-1} + v_k \\Delta t \\sin(\\theta_k) \\\\\n",
    "   \\theta_{k-1} + \\omega_k \\Delta t \\\\\n",
    "   v_k \\\\\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "\n",
    "4. **Predict the Next Covariance ($ P_{k|k-1} $)**:\n",
    "\n",
    "   $$\n",
    "   P_{k|k-1} = F_k P_{k-1} F_k^\\top + Q\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "#### EKF Update Step\n",
    "\n",
    "##### Inputs:\n",
    "\n",
    "- Predicted state estimate: $ \\hat{X}_{k|k-1} $\n",
    "- Predicted covariance: $ P_{k|k-1} $\n",
    "- Measurement vector: $ Z_k $\n",
    "\n",
    "##### Calculations:\n",
    "\n",
    "1. **Measurement Matrix ($ H_k $)**:\n",
    "\n",
    "   Since the measurements directly observe the state variables, $ H_k $ is an identity matrix:\n",
    "\n",
    "   $$\n",
    "   H_k = I_4 = \\begin{bmatrix}\n",
    "   1 & 0 & 0 & 0 \\\\\n",
    "   0 & 1 & 0 & 0 \\\\\n",
    "   0 & 0 & 1 & 0 \\\\\n",
    "   0 & 0 & 0 & 1 \\\\\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "2. **Compute the Kalman Gain ($ K_k $)**:\n",
    "\n",
    "   $$\n",
    "   K_k = P_{k|k-1} H_k^\\top \\left( H_k P_{k|k-1} H_k^\\top + R \\right)^{-1}\n",
    "   $$\n",
    "\n",
    "3. **Update the State Estimate ($ \\hat{X}_k $)**:\n",
    "\n",
    "   $$\n",
    "   \\hat{X}_k = \\hat{X}_{k|k-1} + K_k \\left( Z_k - H_k \\hat{X}_{k|k-1} \\right)\n",
    "   $$\n",
    "\n",
    "4. **Update the Covariance Estimate ($ P_k $)**:\n",
    "\n",
    "   $$\n",
    "   P_k = \\left( I_4 - K_k H_k \\right) P_{k|k-1}\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "#### Summary of the EKF Process\n",
    "\n",
    "##### **Prediction Step**:\n",
    "\n",
    "- **State Prediction**:\n",
    "  $$\n",
    "  \\hat{X}_{k|k-1} = f(\\hat{X}_{k-1}, u_k)\n",
    "  $$\n",
    "  Where $ u_k = [v_l, v_r]^\\top $ are the control inputs.\n",
    "\n",
    "- **Covariance Prediction**:\n",
    "  $$\n",
    "  P_{k|k-1} = F_k P_{k-1} F_k^\\top + Q\n",
    "  $$\n",
    "\n",
    "##### **Update Step**:\n",
    "\n",
    "- **Kalman Gain**:\n",
    "  $$\n",
    "  K_k = P_{k|k-1} H_k^\\top \\left( H_k P_{k|k-1} H_k^\\top + R \\right)^{-1}\n",
    "  $$\n",
    "\n",
    "- **State Update**:\n",
    "  $$\n",
    "  \\hat{X}_k = \\hat{X}_{k|k-1} + K_k \\left( Z_k - H_k \\hat{X}_{k|k-1} \\right)\n",
    "  $$\n",
    "\n",
    "- **Covariance Update**:\n",
    "  $$\n",
    "  P_k = \\left( I_4 - K_k H_k \\right) P_{k|k-1}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "#### Detailed Explanation\n",
    "\n",
    "##### **1. State Variables**\n",
    "\n",
    "- $ x $: Position along the x-axis.\n",
    "- $ y $: Position along the y-axis.\n",
    "- $ \\theta $: Orientation angle of the robot.\n",
    "- $ v $: Linear velocity of the robot.\n",
    "\n",
    "##### **2. Motion Model**\n",
    "\n",
    "The robot's motion model predicts its new position and orientation based on its current state and control inputs (wheel velocities).\n",
    "\n",
    "- **State Transition Function ($ f $)**:\n",
    "  $$\n",
    "  \\begin{cases}\n",
    "  x_{k|k-1} = x_{k-1} + v_k \\Delta t \\cos(\\theta_{k-1}) \\\\\n",
    "  y_{k|k-1} = y_{k-1} + v_k \\Delta t \\sin(\\theta_{k-1}) \\\\\n",
    "  \\theta_{k|k-1} = \\theta_{k-1} + \\omega_k \\Delta t \\\\\n",
    "  v_{k|k-1} = v_k \\\\\n",
    "  \\end{cases}\n",
    "  $$\n",
    "\n",
    "- **Control Inputs**:\n",
    "  - **Linear Velocity ($ v_k $)**:\n",
    "    $$\n",
    "    v_k = \\frac{v_r + v_l}{2}\n",
    "    $$\n",
    "  - **Angular Velocity ($ \\omega_k $)**:\n",
    "    $$\n",
    "    \\omega_k = \\frac{v_r - v_l}{b}\n",
    "    $$\n",
    "\n",
    "##### **3. Jacobian Matrices**\n",
    "\n",
    "- **Jacobian of the Motion Model ($ F_k $)**:\n",
    "  This matrix linearizes the motion model around the current estimate.\n",
    "\n",
    "  $$\n",
    "  F_k = \\begin{bmatrix}\n",
    "  1 & 0 & -v_k \\Delta t \\sin(\\theta_{k-1}) & \\Delta t \\cos(\\theta_{k-1}) \\\\\n",
    "  0 & 1 & v_k \\Delta t \\cos(\\theta_{k-1}) & \\Delta t \\sin(\\theta_{k-1}) \\\\\n",
    "  0 & 0 & 1 & \\Delta t \\\\\n",
    "  0 & 0 & 0 & 1 \\\\\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "- **Measurement Matrix ($ H_k $)**:\n",
    "  Since measurements directly observe the state, $ H_k $ is the identity matrix.\n",
    "\n",
    "  $$\n",
    "  H_k = I_4\n",
    "  $$\n",
    "\n",
    "##### **4. Noise Covariances**\n",
    "\n",
    "- **Process Noise Covariance ($ Q $)**: Represents the uncertainty in the motion model.\n",
    "- **Measurement Noise Covariance ($ R $)**: Represents the uncertainty in the sensor measurements.\n",
    "\n",
    "##### **5. Kalman Gain ($ K_k $)**\n",
    "\n",
    "The Kalman Gain determines how much the measurements influence the updated estimate.\n",
    "\n",
    "$$\n",
    "K_k = P_{k|k-1} H_k^\\top \\left( H_k P_{k|k-1} H_k^\\top + R \\right)^{-1}\n",
    "$$\n",
    "\n",
    "##### **6. Measurement Update**\n",
    "\n",
    "The difference between the actual measurement and the predicted measurement (residual):\n",
    "\n",
    "$$\n",
    "\\text{Residual} = Z_k - H_k \\hat{X}_{k|k-1}\n",
    "$$\n",
    "\n",
    "Updated state estimate:\n",
    "\n",
    "$$\n",
    "\\hat{X}_k = \\hat{X}_{k|k-1} + K_k \\left( Z_k - H_k \\hat{X}_{k|k-1} \\right)\n",
    "$$\n",
    "\n",
    "Updated covariance estimate:\n",
    "\n",
    "$$\n",
    "P_k = \\left( I_4 - K_k H_k \\right) P_{k|k-1}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Notes**\n",
    "- **Units**: All distances are in millimeters (mm), angles in radians, and time in seconds.\n",
    "- **Assumptions**:\n",
    "  - The robot moves on a plane with differential drive kinematics.\n",
    "  - Process and measurement noises are zero-mean Gaussian.\n",
    "  - The system is mildly nonlinear, suitable for EKF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eb99c6",
   "metadata": {},
   "source": [
    "### **Process Noise and Measurement Noise Covariance Matrices Derivation**\n",
    "\n",
    "This section demonstrates the derivation of the covariance matrix for process noise and measurement noise in a Thymio robotic system. The key objectives include:\n",
    "\n",
    "- Collecting data for positional, angular, and velocity measurements.\n",
    "- Computing variances and covariances for these variables.\n",
    "- Formulating a diagonalized covariance matrix for use in Extended Kalman Filter (EKF) implementations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcec91c2",
   "metadata": {},
   "source": [
    "#### **Process Noise Covariance Matrix Derivation**\n",
    "In our system, we see the odometry obtained by the motor speed sensors of Thymio robot as the \"ground truth\". The process noise covariance matrix Q should reflect the uncertainty in the odometry data. we can determine Q by gathering data from the wheel speed sensor and analyzing the noise in the odometry outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e6223d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdmclient import ClientAsync\n",
    "client = ClientAsync()\n",
    "node = await client.wait_for_node()\n",
    "await node.lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "36729c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Timer\n",
    "\n",
    "class RepeatedTimer(object):\n",
    "    def __init__(self, interval, function, *args, **kwargs):\n",
    "        self._timer     = None\n",
    "        self.interval   = interval\n",
    "        self.function   = function\n",
    "        self.args       = args\n",
    "        self.kwargs     = kwargs\n",
    "        self.is_running = False\n",
    "        self.start()\n",
    "\n",
    "    def _run(self):\n",
    "        self.is_running = False\n",
    "        self.start()\n",
    "        self.function(*self.args, **self.kwargs)\n",
    "\n",
    "    def start(self):\n",
    "        if not self.is_running:\n",
    "            self._timer = Timer(self.interval, self._run)\n",
    "            self._timer.start()\n",
    "            self.is_running = True\n",
    "\n",
    "    def stop(self):\n",
    "        self._timer.cancel()\n",
    "        self.is_running = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cc32f1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "acquire_data = True\n",
    "Ts = 0.1\n",
    "thymio_data = []\n",
    "\n",
    "def motors(left, right):\n",
    "    return {\n",
    "        \"motor.left.target\": [left],\n",
    "        \"motor.right.target\": [right],\n",
    "    }\n",
    "\n",
    "def get_data():\n",
    "    thymio_data.append({\"ground\":list(node[\"prox.ground.reflected\"]), \n",
    "                        \"sensor\":list(node[\"prox.ground.reflected\"]),\n",
    "                        \"left_speed\":node[\"motor.left.speed\"],\n",
    "                        \"right_speed\":node[\"motor.right.speed\"]})\n",
    "    \n",
    "\n",
    "if acquire_data:\n",
    "    await node.wait_for_variables() # wait for Thymio variables values\n",
    "    rt = RepeatedTimer(Ts, get_data) # it auto-starts, no need of rt.start()\n",
    "\n",
    "    try:\n",
    "        # time.sleep would not work here, use asynchronous client.sleep method instead\n",
    "        await client.sleep(1)\n",
    "        node.send_set_variables(motors(50, 50))\n",
    "        await client.sleep(10) \n",
    "    finally:\n",
    "        rt.stop() # better in a try/finally block to make sure the program ends!\n",
    "        node.send_set_variables(motors(0, 0))\n",
    "else:\n",
    "    thymio_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "646d8349",
   "metadata": {},
   "outputs": [],
   "source": [
    "wheel_base = 95 # The wheel base of Thymio robot measured by a ruler (mm)\n",
    "\n",
    "# Extract data for motor speed\n",
    "v_l = [x[\"left_speed\"] for x in thymio_data]\n",
    "v_r = [x[\"right_speed\"] for x in thymio_data]\n",
    "\n",
    "# Convert the data into mm/s (conversion factor \"2688.170234\" is derived by \n",
    "# putting a pencil in Thymio robot and measure the trajectory plotted by it with a ruler)\n",
    "v_l = np.array(v_l) / 2.688170243\n",
    "v_r = np.array(v_r) / 2.688170243\n",
    "vel_hat = [0]\n",
    "omg = [0]\n",
    "x_hat = [0]\n",
    "y_hat= [0]\n",
    "theta_hat = [0]\n",
    "\n",
    "for i in range(len(v_l) - 1):\n",
    "    # Update the states\n",
    "    vel_hat.append((v_r[i] + v_l[i]) / 2)\n",
    "    omg.append((v_l[i] - v_r[i]) / wheel_base)\n",
    "    theta_hat.append(theta_hat[-1] + omg[i] * Ts)\n",
    "    x_hat.append(x_hat[-1] + Ts * vel_hat[i] * np.sin(theta_hat[i]))\n",
    "    y_hat.append(y_hat[-1] + Ts * vel_hat[i] * np.cos(theta_hat[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b335faec",
   "metadata": {},
   "source": [
    "<span style=\"color: #2980B9 ;\">\n",
    "<blockquote>\n",
    "    We calculate the variances from 25 because befor 25 time steps it is transient and we don't want the transient data to effect the variances of the steady state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c04580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_theta = np.var(theta_hat[25:])\n",
    "var_vel = np.var(vel_hat[25:])\n",
    "print(\"The velocity variance in m^2/s^2 is {}\".format(var_vel))\n",
    "print(\"The theta variance in rad^2 is {}\".format(var_theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d68ee6",
   "metadata": {},
   "source": [
    "##### **X and Y Variances Derivation**\n",
    "In the following cells, we ask thymio to remain still for 10 seconds and collect the data from wheel speed sensors. We then convert the speed of the left, right motors into the states we need via the dynamic equations of our system, and we calculate the variances for x position and y position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d83bdcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "acquire_data = True\n",
    "Ts = 0.1\n",
    "thymio_data = []\n",
    "\n",
    "def motors(left, right):\n",
    "    return {\n",
    "        \"motor.left.target\": [left],\n",
    "        \"motor.right.target\": [right],\n",
    "    }\n",
    "\n",
    "def get_data():\n",
    "    thymio_data.append({\"ground\":list(node[\"prox.ground.reflected\"]), \n",
    "                        \"sensor\":list(node[\"prox.ground.reflected\"]),\n",
    "                        \"left_speed\":node[\"motor.left.speed\"],\n",
    "                        \"right_speed\":node[\"motor.right.speed\"]})\n",
    "    \n",
    "\n",
    "if acquire_data:\n",
    "    await node.wait_for_variables() # wait for Thymio variables values\n",
    "    rt = RepeatedTimer(Ts, get_data) # it auto-starts, no need of rt.start()\n",
    "\n",
    "    try:\n",
    "        # time.sleep would not work here, use asynchronous client.sleep method instead\n",
    "        await client.sleep(1)\n",
    "        node.send_set_variables(motors(0, 0)) # Thymio remains still for 10 sec\n",
    "        await client.sleep(10) \n",
    "    finally:\n",
    "        rt.stop() # better in a try/finally block to make sure the program ends!\n",
    "        node.send_set_variables(motors(0, 0))\n",
    "else:\n",
    "    thymio_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4528d0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data for motor speed\n",
    "v_l0 = [x[\"left_speed\"] for x in thymio_data]\n",
    "v_r0 = [x[\"right_speed\"] for x in thymio_data]\n",
    "v_l0 = np.array(v_l) / 2.688170243 # in mm/s\n",
    "v_r0 = np.array(v_r) / 2.688170243\n",
    "vel0_hat = [0]\n",
    "omg0 = [0]\n",
    "x_hat0 = [0]\n",
    "y_hat0= [0]\n",
    "theta_hat0 = [0]\n",
    "\n",
    "for i in range(len(v_l) - 1):\n",
    "    # Update the states\n",
    "    vel0_hat.append((v_r0[i] + v_l0[i]) / 2)\n",
    "    omg0.append((v_l0[i] - v_r0[i]) / wheel_base)\n",
    "    theta_hat0.append(theta_hat0[-1] + omg0[i] * Ts)\n",
    "    x_hat0.append(x_hat0[-1] + Ts * vel0_hat[i] * np.sin(theta_hat0[i]))\n",
    "    y_hat0.append(y_hat0[-1] + Ts * vel0_hat[i] * np.cos(theta_hat0[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1ca6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_x = np.var(x_hat0[25:])\n",
    "var_y = np.var(y_hat0[25:])\n",
    "print(\"The x variance in mm^2 is {}\".format(var_x))\n",
    "print(\"The y variance in mm^2 is {}\".format(var_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e964d4b",
   "metadata": {},
   "source": [
    "##### **Process Noise Covariance Matrix Q**\n",
    "We then combine the obtained variances into a diagonal covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4fd116",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.diag([var_x, var_y, var_theta, var_vel]) # Process noise covariance\n",
    "print(\"The process noise covariance matrix Q is: \")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6f34fc",
   "metadata": {},
   "source": [
    "#### **Measurement Noise Covaraince Matrix Derivation**\n",
    "The measurement noise covariance matrix R should reflect the uncertainty in the camera-based pose estimation (from the ArUco marker).\n",
    "##### **Measurement Noise Covariance Matrix Derivation**\n",
    "In the following cell, we detect the position, yaw angle, and velocity via computer vision, and thus gather the data and calculate the variance of each state. Last, we combine them into a diagonal matrix which is the measurement noise covariance matrix R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5746b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_aruco_marker(image, target_id=2):\n",
    "    aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_50)\n",
    "    parameters = cv2.aruco.DetectorParameters()\n",
    "    detector = cv2.aruco.ArucoDetector(aruco_dict, parameters)\n",
    "\n",
    "    corners, ids, _ = detector.detectMarkers(image)\n",
    "\n",
    "    if ids is not None:\n",
    "        for corner, marker_id in zip(corners, ids):\n",
    "            if marker_id[0] == target_id:  # Only process the marker with ID = 2\n",
    "                points = corner[0]\n",
    "                center_y = int(points[:, 0].mean())\n",
    "                center_x = int(points[:, 1].mean())\n",
    "\n",
    "                # Compute angle\n",
    "                vector = points[1] - points[0]\n",
    "                angle = -np.arctan2(vector[1], vector[0])\n",
    "\n",
    "                # Draw the marker and annotate\n",
    "                cv2.polylines(image, [corner.astype(int)], True, (0, 255, 0), 2)\n",
    "                return image, (center_x, center_y), angle\n",
    "\n",
    "    return image, None, None\n",
    "\n",
    "# Main loop\n",
    "cap = cv2.VideoCapture(Camera)\n",
    "Position_x_before = 0\n",
    "Position_y_before = 0\n",
    "previous_time = time.time()\n",
    "\n",
    "# Storage for measurements\n",
    "measurements = []  # To store [x (mm), y (mm), theta (deg), v (mm/s)]\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    processed_image, position, angle = detect_aruco_marker(frame, target_id=2)\n",
    "\n",
    "    if position:\n",
    "        # Convert to millimeters\n",
    "        x_mm = position[0] * pixel_to_mm\n",
    "        y_mm = position[1] * pixel_to_mm\n",
    "        \n",
    "        # Calculate velocity\n",
    "        current_time = time.time()\n",
    "        delta_time = current_time - previous_time\n",
    "        dx = x_mm - Position_x_before\n",
    "        dy = y_mm - Position_y_before\n",
    "        velocity_mm_s = np.sqrt(dx**2 + dy**2) / delta_time\n",
    "        Position_x_before = x_mm\n",
    "        Position_y_before = y_mm\n",
    "        previous_time = current_time\n",
    "\n",
    "        # Append to measurements\n",
    "        measurements.append([x_mm, y_mm, angle, velocity_mm_s])\n",
    "\n",
    "        # Display results on the video feed\n",
    "        cv2.putText(processed_image, f\"x: {x_mm:.4f} m\", (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "        cv2.putText(processed_image, f\"y: {y_mm:.4f} m\", (10, 50),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "        cv2.putText(processed_image, f\"theta: {angle:.4f} rad\", (10, 70),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "        cv2.putText(processed_image, f\"v: {velocity_mm_s:.4f} m/s\", (10, 90),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "    # Show the processed frame\n",
    "    cv2.imshow(\"Aruco Marker Detection\", processed_image)\n",
    "\n",
    "    # Quit with \"l\"\n",
    "    if cv2.waitKey(1) & 0xFF == ord('l'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Calculate covariance matrix\n",
    "measurements = np.array(measurements)\n",
    "R = np.cov(measurements, rowvar=False)  # rowvar=False ensures columns are treated as variables\n",
    "\n",
    "# Simplify to diagonal matrix\n",
    "R_diagonalized = np.diag(np.diag(R))\n",
    "\n",
    "# Print diagonalized covariance matrix\n",
    "print(\"the measuement noise covariance matrix R is: \")\n",
    "print(R_diagonalized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013a12d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for EKF\n",
    "wheel_base = 95 # Distance between wheels (in mm)\n",
    "Ts = 0.1  # Time step\n",
    "R = R_diagonalized # Measurement noise covariance\n",
    "X_hat = np.array([0, 0, 0, 0])  # Initial state: [x, y, theta, v]\n",
    "P = np.eye(4)  # Initial covariance\n",
    "\n",
    "def ekf_predict(X_hat, P, v_l, v_r):\n",
    "    \"\"\"EKF Prediction step.\"\"\"\n",
    "    v_k = (v_r + v_l) / 2\n",
    "    omega_k = (v_r - v_l) / wheel_base\n",
    "    theta_k = X_hat[2]\n",
    "\n",
    "    F = np.array([\n",
    "        [1, 0, -v_k * Ts * np.sin(theta_k), Ts * np.cos(theta_k)],\n",
    "        [0, 1,  v_k * Ts * np.cos(theta_k), Ts * np.sin(theta_k)],\n",
    "        [0, 0, 1,                          Ts],\n",
    "        [0, 0, 0,                           1]\n",
    "    ])\n",
    "\n",
    "    X_hat = np.array([\n",
    "        X_hat[0] + Ts * v_k * np.cos(theta_k),\n",
    "        X_hat[1] + Ts * v_k * np.sin(theta_k),\n",
    "        wrap_angle(X_hat[2] + Ts * omega_k),\n",
    "        v_k\n",
    "    ])\n",
    "\n",
    "    P = F @ P @ F.T + Q\n",
    "    return X_hat, P\n",
    "\n",
    "def ekf_update(X_hat, P, z_k):\n",
    "    \"\"\"EKF Update step.\"\"\"\n",
    "    H = np.eye(4)\n",
    "    K = P @ H.T @ np.linalg.inv(H @ P @ H.T + R)\n",
    "    X_hat = X_hat + K @ (z_k - X_hat)\n",
    "    P = (np.eye(4) - K @ H) @ P\n",
    "    return X_hat, P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeeff84",
   "metadata": {},
   "source": [
    "## **Execution of all parts in a main program**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c958b445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tdmclient.notebook\n",
    "await tdmclient.notebook.start()\n",
    "\n",
    "### Global Parameters\n",
    "PIXEL_TO_MM = pixel_to_mm  # Conversion factor from pixels to millimeters\n",
    "OBSTACLE_THRESHOLD = 2000  # Threshold for obstacle detection (can be adjusted)\n",
    "goal_threshold_mm = 45  # To adjust distance to the goal to switch to the next segment \n",
    "kidnap = 0 # State of \"kidnap\" situation\n",
    "\n",
    "### Trajectory\n",
    "start_points, goal_points, trajectory = A_Dijkstra(matrix, start_position, goal_position)\n",
    "current_segment = 0   # Tracking the current segment\n",
    "robot_position = [start_points[0][0], start_points[0][1]] # Initial position of the robot\n",
    "robot_orientation = 0\n",
    "spLeft = 0\n",
    "spRight = 0\n",
    "acc_values = [0,0,0]\n",
    "\n",
    "def detect_aruco_marker(img, target_id=2):\n",
    "    \"\"\"\n",
    "    Function to detect the position and angle of rotation of the Aruco ID = 2 marker positioned on the robot\n",
    "    \n",
    "    Input : img -> Input image (projected image) \n",
    "          : target_id=2 -> Id number of the Aruco marker\n",
    "    Output : image -> Image with marker indications\n",
    "           : (center_x, center_y) -> Coordinates of the robot (pixels)\n",
    "           : angle -> Angle of rotation of the robot (rad)\n",
    "    \"\"\"\n",
    "\n",
    "    aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_50)\n",
    "    parameters = cv2.aruco.DetectorParameters()\n",
    "    detector = cv2.aruco.ArucoDetector(aruco_dict, parameters)\n",
    "\n",
    "    corners, ids, _ = detector.detectMarkers(img)\n",
    "    if ids is not None:\n",
    "        for corner, marker_id in zip(corners, ids):\n",
    "            if marker_id[0] == target_id:\n",
    "                points = corner[0]\n",
    "                center_x = int(points[:, 0].mean())\n",
    "                center_y = int(points[:, 1].mean())\n",
    "                # Compute angle then normalization between [-π, π]\n",
    "                vector = points[1] - points[0]\n",
    "                angle = np.atan2(vector[1], vector[0])\n",
    "                return img, (center_x, center_y), wrap_angle(angle)\n",
    "    return img, None, None\n",
    "\n",
    "def euclidean_distance(point1, point2):\n",
    "    \"\"\"Calculates the distance between two points.\"\"\"\n",
    "    return np.sqrt((point1[0] - point2[0])**2 + (point1[1] - point2[1])**2)\n",
    "\n",
    "\n",
    "@tdmclient.notebook.sync_var\n",
    "def motors(l_speed, r_speed):\n",
    "    global motor_left_target, motor_right_target\n",
    "    \n",
    "    motor_left_target = l_speed\n",
    "    motor_right_target = r_speed\n",
    "\n",
    "@tdmclient.notebook.sync_var\n",
    "def prox():\n",
    "    global prox_horizontal\n",
    "    # get the prox values   \n",
    "    prox_vals = np.array(prox_horizontal)\n",
    "    return prox_vals\n",
    "\n",
    "@tdmclient.notebook.sync_var\n",
    "def accelerometer():\n",
    "    global acc\n",
    "    # get the prox values   \n",
    "    acc_vals = np.array(acc)\n",
    "    return acc_vals\n",
    "\n",
    "\n",
    "### Main program\n",
    "cap = cv2.VideoCapture(Camera)\n",
    "\n",
    "while current_segment < len(goal_points):\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        print(\"Error with the camera\")\n",
    "        break\n",
    "\n",
    "    if kidnap == 0 :\n",
    "        ## EKF Prediction \n",
    "        # Simulated real inputs for v_l and v_r (left and right wheel speeds)\n",
    "        v_l, v_r = spLeft, spRight \n",
    "        X_hat, P = ekf_predict(X_hat, P, v_l, v_r)\n",
    "        print(\"Prediction :\")\n",
    "        print(X_hat)\n",
    "\n",
    "        ## Aruco Marker Detection\n",
    "        output_size = (Size_img_y, Size_img_x) \n",
    "        projected_frame = apply_homography(frame, homography_matrix, output_size)\n",
    "        processed_frame, detected_position, detected_orientation = detect_aruco_marker(projected_frame, target_id=2)\n",
    "\n",
    "        if detected_position:\n",
    "            # Update the measurement\n",
    "            z_k = [detected_position[0] * PIXEL_TO_MM,\n",
    "                detected_position[1] * PIXEL_TO_MM,\n",
    "                detected_orientation,\n",
    "                0]  # No velocity measurement from the camera\n",
    "            \n",
    "            ## EKF Update\n",
    "            X_hat, P = ekf_update(X_hat, P, np.array(z_k))\n",
    "            print(\"States estimated of the robot :\")\n",
    "            print(X_hat)\n",
    "\n",
    "            ## Check Distance to Current Goal\n",
    "            distance_to_goal = euclidean_distance(X_hat[:2], goal_points[current_segment])\n",
    "            print(goal_points[current_segment])\n",
    "            if distance_to_goal < goal_threshold_mm:\n",
    "                print(f\"Segment {current_segment + 1} atteint.\")\n",
    "                current_segment += 1\n",
    "                continue\n",
    "\n",
    "            ## Astolfi Control\n",
    "            spLeft, spRight = astolfi_control(X_hat[:2], X_hat[2], goal_points[current_segment])\n",
    "\n",
    "            ## Obstacle Avoidance \n",
    "            prox_values = prox()  \n",
    "            # If the robot detects an obstacle\n",
    "            if max(prox_values) > OBSTACLE_THRESHOLD:\n",
    "                spLeft = 0\n",
    "                spRight = 0\n",
    "                spLeft, spRight = obstacle_avoidance(prox_values)\n",
    "\n",
    "            ## Motor Commands\n",
    "            motors([spLeft], [spRight])\n",
    "\n",
    "    ### Kidnapping situation\n",
    "        acc_values = accelerometer()  \n",
    "        # If abnormal acceleration is detected along the robot's y axis\n",
    "        if abs(acc_values[1])>5:\n",
    "            # We stop the robot\n",
    "            motors(0, 0)\n",
    "            current_segment = 0 \n",
    "            spLeft = 0\n",
    "            spRight = 0\n",
    "            # The robot is in a kidnap situation\n",
    "            kidnap = 1\n",
    "\n",
    "    else :\n",
    "        acc_values = accelerometer()  \n",
    "        if abs(acc_values[1])<3:\n",
    "            ## Aruco Marker Detection\n",
    "            projected_frame = apply_homography(frame, homography_matrix, output_size)\n",
    "            processed_frame, detected_position, detected_orientation = detect_aruco_marker(projected_frame, target_id=2)\n",
    "            # If we redetect the robot marker\n",
    "            if detected_position != None:\n",
    "                detected_position = detected_position[::-1]\n",
    "                # We apply the A* Dijkstra algorithm again, taking the robot's current position as the starting point\n",
    "                start_points, goal_points, trajectory = A_Dijkstra(matrix, detected_position, goal_position)\n",
    "                robot_position = [start_points[0][0], start_points[0][1]]\n",
    "                robot_orientation = detected_orientation\n",
    "                # The robot is no longer in a kidnap situation\n",
    "                kidnap = 0\n",
    "\n",
    "\n",
    "    ## Video Display\n",
    "    cv2.putText(processed_frame, f\"Goal: ({goal_points[current_segment][0]:.1f}, {goal_points[current_segment][1]:.1f})\", \n",
    "                (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "    cv2.putText(processed_frame, f\"Robot: ({X_hat[0]:.1f}, {X_hat[1]:.1f})\", \n",
    "                (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "    cv2.putText(processed_frame, f\"Orientation: {X_hat[2]:.2f}\", \n",
    "                (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "    cv2.imshow(\"Robot Navigation\", processed_frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('l'):\n",
    "        break\n",
    "\n",
    "        \n",
    "# Stop motors and release resources\n",
    "motors(0, 0)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f934fdf",
   "metadata": {},
   "source": [
    "## **Videos**\n",
    "\n",
    "### Test Global and Local Navigation\n",
    "<video controls>\n",
    "  <source src=\"videos/Global_Local_nav.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\n",
    "### Test obstruction camera and kidnapping situation\n",
    "<video controls>\n",
    "  <source src=\"videos/Kidnap.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5526103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop motors\n",
    "motors(0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9284c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop connection with Thymio\n",
    "await tdmclient.notebook.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a157c561",
   "metadata": {},
   "source": [
    "## **Conclusion**\n",
    "\n",
    "We successfully completed this project, meeting all the requirements specified in the brief. With more time, we could have improved the video display or implemented a more efficient optimal path method (such as Visibility Graphs or other techniques). The key to the success of this project was the communication between the different parties, which allowed us to make quicker progress on certain aspects.\n",
    "\n",
    "This has been a very intense and highly interesting project, as it allowed us to explore all the topics covered in class through a concrete application. We would like to thank everyone who assisted us during this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c9df4b",
   "metadata": {},
   "source": [
    "## **References used for this project**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823128b3",
   "metadata": {},
   "source": [
    "- Computer Vision Tutorials (from moodle week 10)\n",
    "- Morphological Transformation (Image Processing I course)\n",
    "- A* Dijkstra : Documents and codes from exercices session 5\n",
    "- Astolfi Controller : Documents from lesson 1\n",
    "- Local Avoidance with ANN : Documents and codes from exercices session 3\n",
    "- Kalman Filter : https://youtu.be/wLlG66W1uQI?si=UYZdBWXx42qzs7Yx, https://youtube.com/playlist?list=PLn8PRpmsu08rSg2j8fBt4JsnBkMfiIhLI&si=mb7H4dLKVYNNxl0A"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
